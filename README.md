# Multi-Fidelity Graph Neural Networks (MFGNN) for Flood Hazard Mapping

> Reference implementation accompanying the paper **â€œMulti-fidelity graph neural networks for efficient and accurate flood hazard mapping.â€** Environmental Modelling & Software, 193 (2025) 106654. DOI: 10.1016/j.envsoft.2025.106654.  îˆ€fileciteîˆ‚turn0file0îˆ

This repository provides code to train and evaluate a **two-stage, multi-fidelity GNN (MFGNN)** that combines many inexpensive **lowâ€‘fidelity (LF)** simulations with a small number of **highâ€‘fidelity (HF)** simulations to produce accurate, highâ€‘resolution flood hazard maps. The approach operates on **unstructured meshes** and uses **graph neural networks** (MeshGraphNet) for message passing on kâ€‘NN graphs.

---

## âœ¨ Highlights
- **Hierarchical LFâ†’HF learning**: train an LF GNN on coarse meshes; upsample predictions; train an HF GNN to learn residual corrections.
- **Unstructured-mesh native**: no rasterization required; works directly with HECâ€‘RASâ€‘style meshes.
- **Strong accuracy vs. compute**: achieves lower MAE/RRMSE and higher CSI than singleâ€‘fidelity GNNs under the same compute budget.  îˆ€fileciteîˆ‚turn0file0îˆ

---

## ğŸ—‚ï¸ Repository Layout

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ constants.py                 # Central experiment configuration (paths, hyperparameters)
â”œâ”€â”€ utils.py                     # Data I/O, graph building, trainers, model loading, LFâ†’HF upsampling
â”œâ”€â”€ train.py                     # Endâ€‘toâ€‘end training: LF stage then HF stage
â”œâ”€â”€ inference.py                 # Evaluation, metrics, and plotting (after training)
â””â”€â”€ scripts/
    â”œâ”€â”€ run_hecras_batch.py      # (Optional) HECâ€‘RAS automation & extraction tooling
    â””â”€â”€ synth_events.py          # (Optional) Synthetic hydrograph/hyetograph generation
```

> The code expects three main entry points: `train.py`, `inference.py`, and shared utilities in `utils.py`. Edit `constants.py` to point at your data, checkpoints, and experiment settings.

---

## ğŸš€ Quick Start

### 1) Environment
We recommend **Python 3.10+** with CUDAâ€‘enabled PyTorch.

```bash
# Create a fresh environment
conda create -n mfgnn python=3.10 -y
conda activate mfgnn

# Core deps (choose versions compatible with your CUDA/PyTorch stack)
pip install torch dgl -f https://data.dgl.ai/wheels/cu$(nvcc --version | awk '/release/ {gsub(/,/, "", $5); print substr($5,3,2)"0"}')/repo.html

# Scientific / utils
pip install numpy scipy scikit-learn pandas h5py matplotlib tqdm

# NVIDIA Modulus (for MeshGraphNet) â€“ pick a version compatible with your CUDA/PyTorch
pip install nvidia-modulus

# Optional: experiment tracking
pip install wandb
```

> **Note:** If `dgl` wheel resolution is tricky, consult the [DGL installation guide](https://www.dgl.ai/pages/start.html) for the exact CUDA/PyTorch matrix, or install the CPU wheel for experimentation.

### 2) Configure paths & hyperparameters
Edit `constants.py` to set data directories, output checkpoints, batch sizes, LF/HF sample counts, learning rates, etc.

Key fields:
- `ckpt_path`, `ckpt_name_lf`, `ckpt_name_hf`
- `data_dir`, `test_dir`, `cache_dir`, `results_dir`
- `input_dim_nodes`, `input_dim_edges`, `output_dim`
- `epochs_LF`, `epochs_HF`, `N_LF`, `N_HF`, `N_Test`

### 3) Prepare data
The code expects tabâ€‘delimited text files for each simulation (per prefix):
- `{prefix}_XY.txt` â€“ coordinates (NÃ—2)  
- `{prefix}_CE.txt` â€“ elevation (NÃ—1)  
- `{prefix}_CA.txt` â€“ cell area (NÃ—1)  
- `{prefix}_CS.txt` â€“ slope (NÃ—1)  
- `{prefix}_A.txt`  â€“ aspect (NÃ—1)  
- `{prefix}_CU.txt` â€“ curvature (NÃ—1)  
- `{prefix}_WD_{ID}.txt` â€“ max water depth time series (TÃ—N)  
- `{prefix}_VX_{ID}.txt`, `{prefix}_VY_{ID}.txt` â€“ velocity components (TÃ—N)  
- `{prefix}_US_InF_{ID}.txt` â€“ inflow hydrograph (TÃ—2: time, value)

`utils.py` computes LF/HF graphs, kâ€‘NN edges, edge features, and normalization stats. Test graphs reuse saved stats to ensure consistent scaling across splits.

> Optional scripts in `scripts/` show how to (a) automate HECâ€‘RAS runs and HDF5 extraction, and (b) generate synthetic events by scaled cloning of base hydro/ hyetographs.

### 4) Train
```bash
python train.py
```
- Trains **LF** model first; saves `normalization_params_LF.json` and the best LF checkpoint.  
- Builds HF training graphs, **upsamples LF predictions to HF**, appends as a feature, then trains the **HF** model.  
- Periodically saves checkpoints and perâ€‘epoch losses to `ckpt_path`.

### 5) Evaluate & Plot
```bash
python inference.py
```
- Loads best LF/HF models and HF stats.  
- Builds HF test graphs and augments features using LF predictions.  
- Computes **MAE, MSE, RMSE, RRMSE, CSI@0.05/0.3, Continuous CSI**, and perâ€‘sample inference times.  
- Saves images for **Ground Truth**, **Prediction**, **Upsampled LF**, and **Error** maps; also saves **nodeâ€‘wise error histograms** and `.npy` arrays for reproducibility.

---

## ğŸ“¦ Data Expectations & Naming

- **Prefixes:** `M80_*` for lowâ€‘fidelity (coarse) and `M10_*` for highâ€‘fidelity (fine) simulations (default).  
- **IDs:** Simulation identifiers (e.g., `T001`, `T002`, â€¦) must be consistent across LF/HF datasets for paired upsampling.
- **Normalization:** Training stores min/max stats per feature; test/val strictly reuse them.

> See the paper for node/edge feature definitions, hydrograph descriptors, and CSI metric details.  îˆ€fileciteîˆ‚turn0file0îˆ

---

## ğŸ“Š Reported Benefits (from the paper)
- **Iowa River (fluvial)**: MFGNN lowers MAE and boosts CSI vs. a singleâ€‘fidelity GNN trained under the same compute budget.  
- **White River (pluvial+fluvial)**: MFGNN yields markedly better MAE/RRMSE/CSI despite far fewer HF runs, demonstrating strong efficiency.  îˆ€fileciteîˆ‚turn0file0îˆ

---

## ğŸ§ª Repro Tips
- Fix seeds in `constants.py` for repeatability.  
- Cache processed graphs via `cache_dir` to avoid repeated preprocessing.  
- Start with smaller `N_LF`, `N_HF`, and a reduced hidden size for quick smokeâ€‘tests.  
- If memoryâ€‘bound on very large meshes, reduce batch size or processor depth (GN blocks).

---

## ğŸ“š Citation

If you use this repository, please cite the paper:

```bibtex
@article{Taghizadeh2025MFGNN,
  title   = {Multi-fidelity graph neural networks for efficient and accurate flood hazard mapping},
  author  = {Taghizadeh, Mehdi and Zandsalimi, Zanko and Shafiee-Jood, Majid and Alemazkoor, Negin},
  journal = {Environmental Modelling and Software},
  volume  = {193},
  pages   = {106654},
  year    = {2025},
  doi     = {10.1016/j.envsoft.2025.106654}
}
```

---

## ğŸ“„ License
Unless otherwise noted, this work is released under the **Apache-2.0** license (to align with NVIDIA Modulus usage). Adjust if your project uses a different license.

---

## ğŸ™‹ Support & Contact
Questions or issues? Please open a GitHub issue. For paperâ€‘related inquiries, contact the corresponding author listed in the manuscript.  îˆ€fileciteîˆ‚turn0file0îˆ
# MFGNN_Flood_Mapping